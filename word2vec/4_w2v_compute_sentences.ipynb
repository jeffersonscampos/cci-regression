{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num = str(sys.argv[1])\n",
    "num = 0\n",
    "chunck = 50 * 1000\n",
    "skiprows = (int(num) * chunck) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 1, 2, 3], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evo1 = pd.read_csv('data/narratives.csv.gz', compression='gzip', nrows=chunck, skiprows=skiprows, header=None)\n",
    "print(evo1.columns)\n",
    "evo1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process contents: 46496\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/portuguese.pickle')\n",
    "reg_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#sentences_list = evo1.apply(lambda row: sent_tokenizer.tokenize(row[3]), axis=1, ignore_failures=True)\n",
    "sentences_list = []\n",
    "for index, row in evo1.iterrows():\n",
    "    if 'Evolu' in row[3]:\n",
    "        sentences_list.append(sent_tokenizer.tokenize(row[3]))\n",
    "\n",
    "print(\"Process contents:\", len(sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process sentences: 313766\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sentence in sentences_list:\n",
    "    sentences.extend(sentence)\n",
    "\n",
    "print(\"Process sentences:\", len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process tokens: 313766\n"
     ]
    }
   ],
   "source": [
    "sentences_tokens = []\n",
    "for sentence in sentences:\n",
    "    tokens = reg_tokenizer.tokenize(sentence)\n",
    "    sentences_tokens.append( [remove_accents(w.lower()) for w in tokens])# if w not in stopwords] )\n",
    "    \n",
    "print(\"Process tokens:\", len(sentences_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"sentences_clean.txt\", \"wb\") as wfp:   #Pickling\n",
    "    pickle.dump(sentences_tokens, wfp)\n",
    "    wfp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
